
# Security and Privacy in Deep Learnig
This is a paper list concerning topics on the trust and dependencies of deep learning: adversarial deep learning and privacy-preserving deep learning. A list of adversarial machine learning papers is also provided for reference.


## Table of Contents

 - [Adversarial Deep Learning](#Adversarial Deep Learning)
 - [Privacy-preserving Deep Learning](#Privacy-preserving Deep Learning)
 - [Machine Learning](#Machine Learning)
 



## Adversarial Deep Learning

### Adversarial Attacks
* [Audio Adversarial Examples: Targeted Attacks on Speech-to-Text](https://arxiv.org/abs/1801.01944) - Nicholas Carlini et al., 2018
* [The Limitations of Deep Learning in Adversarial Settings](https://ieeexplore.ieee.org/abstract/document/7467366/) - Nicolas Papernot  et al., 2016
* [Robust Physical-World Attacks on Deep Learning Models](https://arxiv.org/abs/1707.08945) - Kevin Eykholt et al., 2018
* [Explaining and Harnessing Adversarial Examples](https://arxiv.org/abs/1412.6572) - Ian J. Goodfellow et al., 2015
* [DeepFool: A Simple and Accurate Method to Fool Deep Neural Networks](https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/Moosavi-Dezfooli_DeepFool_A_Simple_CVPR_2016_paper.html) - Seyed-Mohsen Moosavi-Dezfooli et al., 2016
* [Adversarial examples in the physical world](https://arxiv.org/abs/1607.02533) - Alexey Kurakin et al., 2017
* [Adversarial Machine Learning at Scale](https://arxiv.org/abs/1611.01236) - Alexey Kurakin et al., 2017
* [Intriguing properties of neural networks](https://arxiv.org/abs/1312.6199) - Christian Szegedy et al., 2014
* [Adversarial Attacks on Neural Network Policies](https://arxiv.org/abs/1702.02284) - Sandy Huang et al., 2017
* [Practical Black-Box Attacks against Machine Learning](https://dl.acm.org/citation.cfm?id=3053009) - Nicolas Papernot et al., 2017
* [Adversarial Examples for Malware Detection](https://link.springer.com/chapter/10.1007/978-3-319-66399-9_4) - Kathrin Grosse et al., 2017
* [Transferability in Machine Learning: from Phenomena to Black-Box Attacks using Adversarial Samples] (https://arxiv.org/abs/1605.07277) - Nicolas Papernot et al., 2016
* [Towards Evaluating the Robustness of Neural Networks](https://nicholas.carlini.com/papers/2017_sp_nnrobustattacks.pdf) - Nicholas Carlini et al., 2017
* [Boosting Adversarial Attacks with Momentum](http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/4098.pdf) CYinpeng Dong et al., 2018
* [Synthesizing Robust Adversarial Examples](https://arxiv.org/abs/1707.07397) - Anish Athalye et al., 2017

### Adversarial Mitigation & Defense

* [Learning with a Strong Adversary](https://arxiv.org/abs/1511.03034) - Ruitong Huang et al., 2016
* [Mitigating Evasion Attacks to Deep Neural Networks via Region-based Classification](https://dl.acm.org/citation.cfm?id=3134606) - 
Xiaoyu Cao et al., 2017
* [Adversarial Example Defenses: Ensembles of Weak Defenses are not Strong](https://www.usenix.org/system/files/conference/woot17/woot17-paper-he.pdf) - Warren He et al., 2017
* [Distillation as a Defense to Adversarial Perturbations Against Deep Neural Networks ](https://ieeexplore.ieee.org/abstract/document/7546524/) - Nicolas Papernot et al., 2016
* [Understanding Adversarial Training: Increasing Local Stability of Neural Nets through Robust Optimization](https://arxiv.org/abs/1511.05432) - Uri Shaham et al., 2015.
* [Adversarial Examples Are Not Easily Detected: Bypassing Ten Detection Methods](https://dl.acm.org/citation.cfm?id=3140444) - 
Nicholas Carlini et al., 2017
* [Towards Deep Learning Models Resistant to Adversarial Attacks](https://arxiv.org/abs/1706.06083) - Aleksander Madry et al., 2017
* [Towards Deep Neural Network Architectures Robust to Adversarial Examples](https://arxiv.org/abs/1412.5068) - Shixiang Gu et al., 2015
* [Analysis of classifiers robustness to adversarial perturbation](https://link.springer.com/article/10.1007/s10994-017-5663-3) - Alhussein Fawzi et al., 2018
* [Towards Robust Deep Neural Networks with BANG](https://arxiv.org/abs/1612.00138) - Andras Rozsa et al., 2018
* [Certifying Some Distributional Robustness with Principled Adversarial Training](https://arxiv.org/abs/1710.10571) - Aman Sinha et al., 2017
* [Adversarial Logit Pairing](https://arxiv.org/abs/1803.06373) Harini Kannan et al., 2018
* [Efficient Defenses Against Adversarial Attacks](https://dl.acm.org/citation.cfm?id=3140449) - Valentina Zantedeschi et al., 2017



## Privacy-Preserving Deep Learning

* [On the Protection of Private Information in Machine Learning Systems: Two Recent Approches](https://ieeexplore.ieee.org/abstract/document/8049647/) - Martín Abadi et al., 2017
* [Deep Learning with Differential Privacy](https://dl.acm.org/citation.cfm?id=2978318) - Martin Abadi et al., 2016
* [Privacy_preserving Deep Learning](https://dl.acm.org/citation.cfm?id=2813687) - Reza Shokri et al., 2015

## Machine Learning


### (Deep) Machine Learning
* [Distilling the Knowledge in a Neural Network](https://arxiv.org/abs/1503.02531) - Geoffrey Hinton et al., 2015
* [Large Scale Distributed Deep Networks](http://papers.nips.cc/paper/4687-large-scale-distributed-deep-networks) - Jeffrey Dean et al., 2012
 * [Very Deep Convolutional Networks for Large-Scale Image Recognition](https://arxiv.org/abs/1409.1556) - Karen Simonyan et al., 2015
 * [Spatial Transformer Networks](http://papers.nips.cc/paper/5854-spatial-transformer-networks) - Max Jaderberg et al., 2015

### Adversarial Machine Learning
* [Adversarial Machine Learning](https://dl.acm.org/citation.cfm?id=2046692) - Ling Huang et al., 2011
* [Adversarial Learning](https://dl.acm.org/citation.cfm?id=1081950) - Daniel Lowd et al., 2005
* [Stealing Hyperparameters in Machine Learning](https://arxiv.org/abs/1802.05351) - Binghui Wang et al., 2018
* [Can Machine Learning be Secure?](https://dl.acm.org/citation.cfm?id=1128824) - Marco Barreno et al., 2006
* [Adversarial Classification](https://dl.acm.org/citation.cfm?id=1014066) - Nilesh Dalvi et al., 2004
* [Adversarial Active Learning](https://dl.acm.org/citation.cfm?id=2666656/) - Brad Miller 2014
* [Adversarial learning: A critical review and active learning study ](https://ieeexplore.ieee.org/abstract/document/8168163/
) - D.J. Miller et al., 2017
* [Stealing	Machine	Learning	Models	via	Prediction	APIs](https://www.usenix.org/system/files/conference/usenixsecurity16/sec16_paper_tramer.pdf) - Florian Tramèr et al., 2016
 * [Bounding an Attack’s Complexity for a Simple Learning Model](https://people.eecs.berkeley.edu/~adj/publications/paper-files/sysml2006-attack.pdf) - Blaine Nelson et al., 2006
  * [Nightmare at test time: robust learning by feature deletion](https://dl.acm.org/citation.cfm?id=1143889) - 
Amir Globerson et al., 2006
 * [Evasion Attacks against Machine Learning at Test Time](https://link.springer.com/chapter/10.1007/978-3-642-40994-3_25) -Battista Biggio et al., 2013



### Privacy-preserving Machine Learning
* [Learning in a Large Function Space: Privacy-Preserving Mechanisms for SVM Learning](https://arxiv.org/abs/0911.5708
) - Benjamin I. P. Rubinstein et al., 2009
* [Revealing information while preserving privacy](https://dl.acm.org/citation.cfm?id=773173) - Irit Dinur et al., 2003
* [Privacy-preserving logistic regression](papers.nips.cc/paper/3486-privacy-preserving-logistic-regression) - Kamalika Chaudhuri et al., 2008
 * [A firm foundation for private data analysis](https://dl.acm.org/citation.cfm?id=1866758) - Cynthia Dwork 2011
 * [P4P: Practical Large-Scale Privacy-Preserving Distributed Computation Robust against Malicious Users
](static.usenix.org/legacy/events/sec10/tech/full_papers/Duan.pdf) - Yitao Duan et al., 2010

























## Licenses
License

[![CC0](http://i.creativecommons.org/p/zero/1.0/88x31.png)](http://creativecommons.org/publicdomain/zero/1.0/)

To the extent possible under law, [Wenqi Wei](https://wenqiwei789.github.io/Homepage/) has waived all copyright and related or neighboring rights to this work.
